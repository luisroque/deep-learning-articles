{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scheduled-brake",
   "metadata": {
    "id": "scheduled-brake",
    "papermill": {
     "duration": 0.027712,
     "end_time": "2021-05-18T18:30:16.034443",
     "exception": false,
     "start_time": "2021-05-18T18:30:16.006731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-software",
   "metadata": {
    "id": "equivalent-software",
    "papermill": {
     "duration": 0.026853,
     "end_time": "2021-05-18T18:30:16.087699",
     "exception": false,
     "start_time": "2021-05-18T18:30:16.060846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation [[1]](https://arxiv.org/abs/1609.08144). Its strength comes from the fact that it learns the mapping directly from input text to associated output text. It has been proven to be more effective than traditional phrase-based machine translation, which requires much more effort to design the model. On the other hand, NMT models are costly to train, especially on large-scale translation datasets. They are also significantly slower at inference time due to the large number of parameters used. Other limitations are its robustness when translating rare words and failing to translate all parts of the input sentence. To overcome these problems, there are already some solutions, such as using the attention mechanism to copy rare words [[2]](https://arxiv.org/abs/1412.2007).\n",
    "\n",
    "Typically, NMT models follow the common sequence-to-sequence learning architecture. It consists of an encoder and a decoder Recurrent Neural Networks (RNN) (for a simpler example on how to set up an RNN, see [[3]](https://towardsdatascience.com/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d)). The encoder transforms the input sentence into a list of vectors, one vector per input. Given this list, the decoder produces one output at a time until the special end-of-sentence token is produced.\n",
    "\n",
    "Our task is to produce translations in Portuguese for input sentences in English, using a medium-size corpus of example pairs. We build our NMT model using a sequence to sequence architecture. For the encoder RNN, we use a pre-trained embedding, a token-based text embedding trained on English Google News 200B corpus [[4]](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128). On the other hand, we train our own embedding in the decoder RNN, with a vocabulary size set to the number of unique Portuguese words in the corpus. Due to the complex architecture of the model, we implement a custom training loop to train our model.\n",
    "\n",
    "This article uses a dataset that consists of 170,305 sentence pairs in English and Portuguese [[5]](https://www.kaggle.com/luisroque/engpor-sentence-pairs). The data comes from Tatoeba.org, a large database of example sentences translated into many languages by volunteers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-paragraph",
   "metadata": {
    "id": "vocational-paragraph",
    "papermill": {
     "duration": 0.026229,
     "end_time": "2021-05-18T18:30:16.139985",
     "exception": false,
     "start_time": "2021-05-18T18:30:16.113756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-greek",
   "metadata": {
    "id": "retained-greek",
    "papermill": {
     "duration": 0.026596,
     "end_time": "2021-05-18T18:30:16.192758",
     "exception": false,
     "start_time": "2021-05-18T18:30:16.166162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We start by adding two special tokens to every sentence in Portuguese, a `<start>` and `<end>` tokens. They are used to signal the beginning and end of a sentence to the decode RNN. Next, we tokenize the Portuguese sentences and pad the end of the sentences with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surgical-planner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:16.249019Z",
     "iopub.status.busy": "2021-05-18T18:30:16.247520Z",
     "iopub.status.idle": "2021-05-18T18:30:22.178157Z",
     "shell.execute_reply": "2021-05-18T18:30:22.177532Z"
    },
    "id": "surgical-planner",
    "papermill": {
     "duration": 5.959878,
     "end_time": "2021-05-18T18:30:22.178308",
     "exception": false,
     "start_time": "2021-05-18T18:30:16.218430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "analyzed-borough",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:22.236412Z",
     "iopub.status.busy": "2021-05-18T18:30:22.235736Z",
     "iopub.status.idle": "2021-05-18T18:30:22.239099Z",
     "shell.execute_reply": "2021-05-18T18:30:22.238679Z"
    },
    "id": "analyzed-borough",
    "papermill": {
     "duration": 0.034615,
     "end_time": "2021-05-18T18:30:22.239211",
     "exception": false,
     "start_time": "2021-05-18T18:30:22.204596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "institutional-thumbnail",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:22.300396Z",
     "iopub.status.busy": "2021-05-18T18:30:22.299885Z",
     "iopub.status.idle": "2021-05-18T18:30:39.292372Z",
     "shell.execute_reply": "2021-05-18T18:30:39.291839Z"
    },
    "id": "institutional-thumbnail",
    "outputId": "4642e290-bcf0-4e53-af7a-7884105a1c43",
    "papermill": {
     "duration": 17.027365,
     "end_time": "2021-05-18T18:30:39.292491",
     "exception": false,
     "start_time": "2021-05-18T18:30:22.265126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eng-por.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-104781782c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_EXAMPLES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng-por.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_examples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNUM_EXAMPLES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eng-por.txt'"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 100000\n",
    "data_examples = []\n",
    "with open('eng-por.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(data_examples) < NUM_EXAMPLES:\n",
    "            data_examples.append(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(data_examples, columns=['all'])\n",
    "df = df['all'].str.split('\\t', expand=True)\n",
    "\n",
    "df.columns = columns=['english', 'portuguese', 'rest']\n",
    "\n",
    "df = df[['english', 'portuguese']]\n",
    "\n",
    "df['portuguese'] = df.apply(lambda row: \"<start> \" + preprocess_sentence(row['portuguese']) + \" <end>\", axis=1)\n",
    "df['english'] = df.apply(lambda row: preprocess_sentence(row['english']), axis=1)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "tokenizer.fit_on_texts(df['portuguese'].to_list())\n",
    "\n",
    "df['portuguese_tokens'] = df.apply(lambda row: \n",
    "                        tokenizer.texts_to_sequences([row['portuguese']])[0], axis=1)\n",
    "\n",
    "# Print 5 examples from each language\n",
    "\n",
    "idx = np.random.choice(df.shape[0],5)\n",
    "\n",
    "for i in idx:\n",
    "    print('English')\n",
    "    print(df['english'][i])\n",
    "    \n",
    "    print('\\nportuguese')\n",
    "    print(df['portuguese'][i])\n",
    "    print(df['portuguese_tokens'][i])\n",
    "    print('\\n----\\n')\n",
    "\n",
    "portuguese_tokens = tf.keras.preprocessing.sequence.pad_sequences(df['portuguese_tokens'].to_list(),\n",
    "                                                 padding='post',\n",
    "                                                 value=0)\n",
    "\n",
    "portuguese_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-volume",
   "metadata": {
    "id": "minor-volume",
    "papermill": {
     "duration": 0.026363,
     "end_time": "2021-05-18T18:30:39.345735",
     "exception": false,
     "start_time": "2021-05-18T18:30:39.319372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Pre-Trained Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-surgery",
   "metadata": {
    "id": "guilty-surgery",
    "papermill": {
     "duration": 0.026322,
     "end_time": "2021-05-18T18:30:39.398660",
     "exception": false,
     "start_time": "2021-05-18T18:30:39.372338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For both the encoder and decoder RNNs, we need to define embedding layers to turn our indices of words into dense vectors of fixed size. For the decoder RNN, we trained our own embedding. For the encoder RNN, we used a pre-trained English word embedding from Tensorflow Hub. It is a token-based text embedding trained on the English Google News 200B corpus. It allows us to leverage a word representation trained on a very large corpus, following the principle of Transfer Learning (see [[6]](https://towardsdatascience.com/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43) for an extended definition and a Transfer Learning application to computer vision). We padded the English sentences before feeding them into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-mineral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:39.458297Z",
     "iopub.status.busy": "2021-05-18T18:30:39.457691Z",
     "iopub.status.idle": "2021-05-18T18:30:50.111776Z",
     "shell.execute_reply": "2021-05-18T18:30:50.111338Z"
    },
    "id": "genuine-mineral",
    "outputId": "9fa5dc74-3e23-4e2b-9681-072569cdbffc",
    "papermill": {
     "duration": 10.686685,
     "end_time": "2021-05-18T18:30:50.111918",
     "exception": false,
     "start_time": "2021-05-18T18:30:39.425233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load embedding module from Tensorflow Hub\n",
    "\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\", \n",
    "                                 output_shape=[128], input_shape=[], dtype=tf.string)\n",
    "\n",
    "# Test the layer\n",
    "\n",
    "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fluid-extraction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:50.172893Z",
     "iopub.status.busy": "2021-05-18T18:30:50.172348Z",
     "iopub.status.idle": "2021-05-18T18:30:50.175533Z",
     "shell.execute_reply": "2021-05-18T18:30:50.175906Z"
    },
    "id": "fluid-extraction",
    "papermill": {
     "duration": 0.036477,
     "end_time": "2021-05-18T18:30:50.176029",
     "exception": false,
     "start_time": "2021-05-18T18:30:50.139552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxlen = 13\n",
    "\n",
    "def split_english(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (tf.strings.split(x, sep=' '), y))\n",
    "    return dataset\n",
    "\n",
    "def filter_func(x,y):\n",
    "    return (tf.math.less_equal(len(x),maxlen)) \n",
    "\n",
    "def map_maxlen(x, y):\n",
    "    paddings = [[0, maxlen - tf.shape(x)[0]], [0, 0]]\n",
    "    x = tf.pad(x, paddings, 'CONSTANT', constant_values=0)\n",
    "    return (x, y)\n",
    "\n",
    "def embed_english(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (embedding_layer(x), y))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "celtic-boutique",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:50.242623Z",
     "iopub.status.busy": "2021-05-18T18:30:50.239151Z",
     "iopub.status.idle": "2021-05-18T18:30:51.402664Z",
     "shell.execute_reply": "2021-05-18T18:30:51.403685Z"
    },
    "id": "celtic-boutique",
    "outputId": "fe242d14-ac35-4e20-9d1b-e208584eaaae",
    "papermill": {
     "duration": 1.201035,
     "end_time": "2021-05-18T18:30:51.403905",
     "exception": false,
     "start_time": "2021-05-18T18:30:50.202870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 16), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_strings = df['english'].to_numpy()\n",
    "english_string_train, english_string_valid, portuguese_token_train, portuguese_token_valid = train_test_split(english_strings, portuguese_tokens, train_size=0.8)\n",
    "\n",
    "dataset_train = (embed_english(\n",
    "                        split_english(\n",
    "                            tf.data.Dataset.from_tensor_slices((english_string_train, portuguese_token_train)))\n",
    "                                                                .filter(filter_func))\n",
    "                                                                .map(map_maxlen)\n",
    "                                                                .batch(16))\n",
    "dataset_valid = (embed_english(\n",
    "                        split_english(\n",
    "                            tf.data.Dataset.from_tensor_slices((english_string_valid, portuguese_token_valid)))\n",
    "                                                                .filter(filter_func))\n",
    "                                                                .map(map_maxlen)\n",
    "                                                                .batch(16))\n",
    "\n",
    "dataset_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unique-architect",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:51.476745Z",
     "iopub.status.busy": "2021-05-18T18:30:51.476246Z",
     "iopub.status.idle": "2021-05-18T18:30:51.643144Z",
     "shell.execute_reply": "2021-05-18T18:30:51.642686Z"
    },
    "id": "unique-architect",
    "outputId": "a70618b7-99ba-46cc-c724-abc9bb9adb1f",
    "papermill": {
     "duration": 0.198967,
     "end_time": "2021-05-18T18:30:51.643268",
     "exception": false,
     "start_time": "2021-05-18T18:30:51.444301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 13, 128)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the English data example from the training Dataset\n",
    "list(dataset_train.take(1).as_numpy_iterator())[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acoustic-rwanda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:51.703936Z",
     "iopub.status.busy": "2021-05-18T18:30:51.703171Z",
     "iopub.status.idle": "2021-05-18T18:30:51.813642Z",
     "shell.execute_reply": "2021-05-18T18:30:51.813209Z"
    },
    "id": "acoustic-rwanda",
    "outputId": "ce79d5af-a40f-4151-c1c9-9ddd486f4092",
    "papermill": {
     "duration": 0.142577,
     "end_time": "2021-05-18T18:30:51.813753",
     "exception": false,
     "start_time": "2021-05-18T18:30:51.671176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the portuguese data example Tensor from the validation Dataset\n",
    "list(dataset_valid.take(1).as_numpy_iterator())[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-movement",
   "metadata": {
    "id": "criminal-movement",
    "papermill": {
     "duration": 0.028242,
     "end_time": "2021-05-18T18:30:51.870956",
     "exception": false,
     "start_time": "2021-05-18T18:30:51.842714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Enconder Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-consensus",
   "metadata": {
    "id": "romantic-consensus",
    "papermill": {
     "duration": 0.028762,
     "end_time": "2021-05-18T18:30:51.927765",
     "exception": false,
     "start_time": "2021-05-18T18:30:51.899003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The encoder network is an RNN whose job is to read a sequence of vectors $\\textbf{x}=(x_1,...,x_T)$ into a vector $c$, such that:\n",
    "\n",
    "$$\\begin{aligned}h_t = f(x_t, h_{t-1}) \\\\\n",
    "c = q({h_1,...,h_T}),\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $h_t \\in \\mathbb{R}^n$ is the hidden state at time $t$, $c$ is a vector generated from the sequence of the hidden states and $f$ and $q$ are nonlinear functions.\n",
    "\n",
    "Before defining our encoder network, we introduced a layer that learns the 128-dimensional representation (the size of the embedding space) of the end token for the English corpus. Therefore, the input dimension to the RNN was increased by 1. The RNN consists of a Long Short-Term Memory (LSTM) layer with 1024 units. Padded values are masked in the RNN, so they are simply ignored. The encoder is a multi-output model: it outputs the hidden state and cell states of the LSTM layer. The output of the LSTM layer is not used in a sequence to sequence architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "advisory-handy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:51.993530Z",
     "iopub.status.busy": "2021-05-18T18:30:51.992147Z",
     "iopub.status.idle": "2021-05-18T18:30:51.994938Z",
     "shell.execute_reply": "2021-05-18T18:30:51.994511Z"
    },
    "id": "advisory-handy",
    "papermill": {
     "duration": 0.037553,
     "end_time": "2021-05-18T18:30:51.995047",
     "exception": false,
     "start_time": "2021-05-18T18:30:51.957494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EndTokenLayer(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim=128, **kwargs):\n",
    "        super(EndTokenLayer, self).__init__(**kwargs)\n",
    "        self.end_token_embedding = tf.Variable(initial_value=tf.random.uniform(shape=(embedding_dim,)), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        end_token = tf.tile(tf.reshape(self.end_token_embedding, shape=(1, 1, self.end_token_embedding.shape[0])), [tf.shape(inputs)[0],1,1])\n",
    "        return tf.keras.layers.concatenate([inputs, end_token], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bright-journal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:52.055674Z",
     "iopub.status.busy": "2021-05-18T18:30:52.055146Z",
     "iopub.status.idle": "2021-05-18T18:30:52.061257Z",
     "shell.execute_reply": "2021-05-18T18:30:52.060638Z"
    },
    "id": "bright-journal",
    "papermill": {
     "duration": 0.037929,
     "end_time": "2021-05-18T18:30:52.061372",
     "exception": false,
     "start_time": "2021-05-18T18:30:52.023443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_token_layer = EndTokenLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "future-stocks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:52.130948Z",
     "iopub.status.busy": "2021-05-18T18:30:52.125729Z",
     "iopub.status.idle": "2021-05-18T18:30:53.062479Z",
     "shell.execute_reply": "2021-05-18T18:30:53.062022Z"
    },
    "id": "future-stocks",
    "papermill": {
     "duration": 0.972923,
     "end_time": "2021-05-18T18:30:53.062608",
     "exception": false,
     "start_time": "2021-05-18T18:30:52.089685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(maxlen, 128))\n",
    "x = end_token_layer(inputs)\n",
    "x = tf.keras.layers.Masking(mask_value=0)(x)\n",
    "whole_seq_output, final_memory_state, final_carry_state = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)(x)\n",
    "outputs = (final_memory_state, final_carry_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "binding-explorer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:53.125964Z",
     "iopub.status.busy": "2021-05-18T18:30:53.125146Z",
     "iopub.status.idle": "2021-05-18T18:30:53.134161Z",
     "shell.execute_reply": "2021-05-18T18:30:53.134526Z"
    },
    "id": "binding-explorer",
    "outputId": "72820590-71c7-44e4-9613-0dacbcf5b818",
    "papermill": {
     "duration": 0.043198,
     "end_time": "2021-05-18T18:30:53.134666",
     "exception": false,
     "start_time": "2021-05-18T18:30:53.091468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 13, 128)]         0         \n",
      "_________________________________________________________________\n",
      "end_token_layer_1 (EndTokenL (None, 14, 128)           128       \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                [(None, 14, 512), (None,  1312768   \n",
      "=================================================================\n",
      "Total params: 1,312,896\n",
      "Trainable params: 1,312,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"encoder_model\")\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "confident-brush",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:53.196346Z",
     "iopub.status.busy": "2021-05-18T18:30:53.195824Z",
     "iopub.status.idle": "2021-05-18T18:30:53.309456Z",
     "shell.execute_reply": "2021-05-18T18:30:53.309943Z"
    },
    "id": "confident-brush",
    "papermill": {
     "duration": 0.146453,
     "end_time": "2021-05-18T18:30:53.310103",
     "exception": false,
     "start_time": "2021-05-18T18:30:53.163650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs_eng = list(dataset_train.take(1).as_numpy_iterator())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "possible-postage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:53.373970Z",
     "iopub.status.busy": "2021-05-18T18:30:53.373388Z",
     "iopub.status.idle": "2021-05-18T18:30:54.337223Z",
     "shell.execute_reply": "2021-05-18T18:30:54.336393Z"
    },
    "id": "possible-postage",
    "papermill": {
     "duration": 0.997422,
     "end_time": "2021-05-18T18:30:54.337357",
     "exception": false,
     "start_time": "2021-05-18T18:30:53.339935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory_state, carry_state = encoder_model(inputs_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-health",
   "metadata": {
    "id": "boring-health",
    "papermill": {
     "duration": 0.029053,
     "end_time": "2021-05-18T18:30:54.395914",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.366861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-sector",
   "metadata": {
    "id": "patient-sector",
    "papermill": {
     "duration": 0.02888,
     "end_time": "2021-05-18T18:30:54.454463",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.425583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The attention mechanism that we will be using was proposed by [[7]](https://arxiv.org/pdf/1409.0473.pdf). The main difference of using an attention mechanism is that we increase the expressiveness of the model, especially the encoder component. It no longer needs to encode all the information in the source sentence into a fixed-length vector. The context vector $c_i$ is then computed as:\n",
    "\n",
    "$$c_i = \\sum^{T_x}_{j=1}\\alpha_{ij}h_j$$.\n",
    "\n",
    "The weights $\\alpha_{ij}$ are calculated as \n",
    "\n",
    "$$\\alpha_{ij}=\\frac{\\exp(e_{ij})}{\\sum^{T}_{k=1}\\exp(e_{ik})},$$\n",
    "\n",
    "where $e_{ij}=a(s_{i-1},h_j)$ is the score of how well the inputs around position $j$ and the output at position $i$ match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "indian-match",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:54.520070Z",
     "iopub.status.busy": "2021-05-18T18:30:54.518808Z",
     "iopub.status.idle": "2021-05-18T18:30:54.521120Z",
     "shell.execute_reply": "2021-05-18T18:30:54.521515Z"
    },
    "id": "indian-match",
    "papermill": {
     "duration": 0.038303,
     "end_time": "2021-05-18T18:30:54.521646",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.483343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden_states, cell_states):\n",
    "        hidden_states_with_time = tf.expand_dims(hidden_states, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(hidden_states_with_time) + self.W2(cell_states)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * cell_states\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-athletics",
   "metadata": {
    "id": "lonely-athletics",
    "papermill": {
     "duration": 0.028871,
     "end_time": "2021-05-18T18:30:54.579701",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.550830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Decoder Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-alabama",
   "metadata": {
    "id": "exclusive-alabama",
    "papermill": {
     "duration": 0.029218,
     "end_time": "2021-05-18T18:30:54.638134",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.608916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The decoder is trained to predict the next word $y_t$ given the context vector $c$ and all the previously predicted words ${y_1,...,y_{t-1}}$, such that:\n",
    "\n",
    "$$p(\\textbf{y})=\\prod^T_{t=1}p(y_t|{y_1,...,y_{t-1}},c)$$\n",
    "\n",
    "where $\\textbf{y}=(y_1,...,y_T)$. For this we use an RNN, which means that each conditional probability is modeled as\n",
    "\n",
    "$$p(y_t|{y_1,...,y_{t-1}},c)=g(y_{t-1},s_t,c),$$\n",
    "\n",
    "where $g$ is a nonlinear function and $s_t$ is the hidden state of the RNN.\n",
    "\n",
    "For the decoder RNN, we defined an embedding layer with the vocabulary size set to the number of unique Portuguese tokens. An LSTM layer followed this embedding layer with 1024 units and a Dense layer with a number of units equal to the number of unique Portuguese tokens and no activation function. Notice that despite the considerably shallow network, as we used only one recurrent layer, we end up with almost 24M parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "subjective-briefing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:54.746123Z",
     "iopub.status.busy": "2021-05-18T18:30:54.738573Z",
     "iopub.status.idle": "2021-05-18T18:30:54.748471Z",
     "shell.execute_reply": "2021-05-18T18:30:54.748077Z"
    },
    "id": "subjective-briefing",
    "papermill": {
     "duration": 0.081174,
     "end_time": "2021-05-18T18:30:54.748582",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.667408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "word_index = json.loads(tokenizer.get_config()['word_index'])\n",
    "\n",
    "max_index_value = max(word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "vital-polish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:54.813651Z",
     "iopub.status.busy": "2021-05-18T18:30:54.813160Z",
     "iopub.status.idle": "2021-05-18T18:30:54.816896Z",
     "shell.execute_reply": "2021-05-18T18:30:54.816449Z"
    },
    "id": "vital-polish",
    "papermill": {
     "duration": 0.039324,
     "end_time": "2021-05-18T18:30:54.817001",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.777677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class decoder_RNN(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(decoder_RNN, self).__init__()\n",
    "        self.embed = tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=128, mask_zero=True)\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(1024, return_sequences=True, return_state=True)\n",
    "        self.dense_1 = tf.keras.layers.Dense(max_index_value+1)\n",
    "        self.attention = Attention(1024)\n",
    "\n",
    "    def call(self, inputs, training=False, hidden_state=None, cell_state=None):\n",
    "        context_vector, attention_weights = self.attention(hidden_state, cell_state)\n",
    "        x = self.embed(inputs)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        x, hidden_state, cell_state = self.lstm_1(x)\n",
    "        x = self.dense_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "congressional-begin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:54.881688Z",
     "iopub.status.busy": "2021-05-18T18:30:54.880926Z",
     "iopub.status.idle": "2021-05-18T18:30:54.893430Z",
     "shell.execute_reply": "2021-05-18T18:30:54.893854Z"
    },
    "id": "congressional-begin",
    "papermill": {
     "duration": 0.047773,
     "end_time": "2021-05-18T18:30:54.893981",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.846208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_model = decoder_RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "young-salem",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:54.959280Z",
     "iopub.status.busy": "2021-05-18T18:30:54.958482Z",
     "iopub.status.idle": "2021-05-18T18:30:57.495883Z",
     "shell.execute_reply": "2021-05-18T18:30:57.495225Z"
    },
    "id": "young-salem",
    "outputId": "cdeb8c58-99e8-43a1-9ed3-e12c84042422",
    "papermill": {
     "duration": 2.572051,
     "end_time": "2021-05-18T18:30:57.496043",
     "exception": false,
     "start_time": "2021-05-18T18:30:54.923992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_rnn_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  1789568   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  6819840   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  14330525  \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      multiple                  1051649   \n",
      "=================================================================\n",
      "Total params: 23,991,582\n",
      "Trainable params: 23,991,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model(inputs = tf.random.uniform((16, 1)), hidden_state = memory_state, cell_state = carry_state)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-american",
   "metadata": {
    "id": "covered-american",
    "papermill": {
     "duration": 0.0297,
     "end_time": "2021-05-18T18:30:57.556759",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.527059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-massachusetts",
   "metadata": {
    "id": "impressed-massachusetts",
    "papermill": {
     "duration": 0.029722,
     "end_time": "2021-05-18T18:30:57.616306",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.586584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To train a model with a sequence to sequence architecture, we needed to define a custom training loop. First, we defined a function that split the Portuguese corpus into the input and output tensors fed to the decoder model. Secondly, we created the forward and backward passes of the complete model. We passed the English input into the encoder to get the hidden and cell states of the encoder LSTM. These hidden and cell states are then passed into the decoder, along with the Portuguese inputs. We defined the loss function, calculated between the decoder outputs and the Portuguese output previously split, and the computation of the gradients with respect to the encoder and decoder trainable variables. Finally, we ran the training loop for a defined number of epochs. It iterated through the training dataset, creating the decoder inputs and outputs from the Portuguese sequences. It then computed the gradients and updated the parameters of the model accordingly.\n",
    "\n",
    "The model is quite slow to train, even using GPU. Recall that we did not even stack layers in any of the RNNs, which would reduce our loss but, at the same time, make our model even harder to train. We can see from the plots below that both the training and validation reduced steadily over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "beginning-liverpool",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:57.680965Z",
     "iopub.status.busy": "2021-05-18T18:30:57.680149Z",
     "iopub.status.idle": "2021-05-18T18:30:57.682735Z",
     "shell.execute_reply": "2021-05-18T18:30:57.682324Z"
    },
    "id": "beginning-liverpool",
    "papermill": {
     "duration": 0.036545,
     "end_time": "2021-05-18T18:30:57.682854",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.646309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def portuguese_input_output(data):\n",
    "    return (tf.cast(data[:,0:-1], tf.float32), tf.cast(data[:,1:], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "waiting-rouge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:57.746873Z",
     "iopub.status.busy": "2021-05-18T18:30:57.746143Z",
     "iopub.status.idle": "2021-05-18T18:30:57.748387Z",
     "shell.execute_reply": "2021-05-18T18:30:57.748830Z"
    },
    "id": "waiting-rouge",
    "papermill": {
     "duration": 0.036228,
     "end_time": "2021-05-18T18:30:57.748950",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.712722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "floating-coupon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:57.815609Z",
     "iopub.status.busy": "2021-05-18T18:30:57.814875Z",
     "iopub.status.idle": "2021-05-18T18:30:57.817513Z",
     "shell.execute_reply": "2021-05-18T18:30:57.817117Z"
    },
    "id": "floating-coupon",
    "papermill": {
     "duration": 0.038542,
     "end_time": "2021-05-18T18:30:57.817617",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.779075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def grad(encoder_model, decoder_model, english_input, portuguese_input, portuguese_output, loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = 0\n",
    "        hidden_state, cell_state = encoder_model(english_input)\n",
    "        dec_input = tf.expand_dims([word_index['<start>']] * 16, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, portuguese_output.shape[1]):\n",
    "            predictions = decoder_model(dec_input, hidden_state = hidden_state, cell_state = cell_state)\n",
    "\n",
    "            loss_value += loss(portuguese_output[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(portuguese_output[:, t], 1)\n",
    "\n",
    "        grads = tape.gradient(loss_value, encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
    "    return loss_value, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "transsexual-sucking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:57.885414Z",
     "iopub.status.busy": "2021-05-18T18:30:57.884635Z",
     "iopub.status.idle": "2021-05-18T18:30:57.887329Z",
     "shell.execute_reply": "2021-05-18T18:30:57.886852Z"
    },
    "id": "transsexual-sucking",
    "papermill": {
     "duration": 0.039365,
     "end_time": "2021-05-18T18:30:57.887433",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.848068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(encoder_model, decoder_model, num_epochs, train_dataset, valid_dataset, optimizer, loss, grad_fn):\n",
    "    inputs = (14,)\n",
    "    train_loss_results = []\n",
    "    train_loss_results_valid = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for x, y in train_dataset:\n",
    "            dec_inp, dec_out = portuguese_input_output(y)\n",
    "            loss_value, grads = grad_fn(encoder_model, decoder_model, x, dec_inp, dec_out, loss)\n",
    "            optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
    "            epoch_loss_avg(loss_value)\n",
    "\n",
    "        train_loss_results.append(epoch_loss_avg.result())\n",
    "        \n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch,\n",
    "                                                  epoch_loss_avg.result()))\n",
    "        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
    "    return train_loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "decent-bridal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-18T18:30:57.957436Z",
     "iopub.status.busy": "2021-05-18T18:30:57.956350Z",
     "iopub.status.idle": "2021-05-19T01:21:40.259444Z",
     "shell.execute_reply": "2021-05-19T01:21:40.260117Z"
    },
    "id": "decent-bridal",
    "outputId": "7bf1e9f9-3e07-4397-b24d-d8786959daf5",
    "papermill": {
     "duration": 24642.342789,
     "end_time": "2021-05-19T01:21:40.260344",
     "exception": false,
     "start_time": "2021-05-18T18:30:57.917555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 23.192\n",
      "Time taken for 1 epoch 758.72 sec\n",
      "\n",
      "Epoch 001: Loss: 19.355\n",
      "Time taken for 1 epoch 749.01 sec\n",
      "\n",
      "Epoch 002: Loss: 17.883\n",
      "Time taken for 1 epoch 748.91 sec\n",
      "\n",
      "Epoch 003: Loss: 16.368\n",
      "Time taken for 1 epoch 749.56 sec\n",
      "\n",
      "Epoch 004: Loss: 12.706\n",
      "Time taken for 1 epoch 750.83 sec\n",
      "\n",
      "Epoch 005: Loss: 7.722\n",
      "Time taken for 1 epoch 746.72 sec\n",
      "\n",
      "Epoch 006: Loss: 5.026\n",
      "Time taken for 1 epoch 744.72 sec\n",
      "\n",
      "Epoch 007: Loss: 3.540\n",
      "Time taken for 1 epoch 747.66 sec\n",
      "\n",
      "Epoch 008: Loss: 2.604\n",
      "Time taken for 1 epoch 748.59 sec\n",
      "\n",
      "Epoch 009: Loss: 2.002\n",
      "Time taken for 1 epoch 755.00 sec\n",
      "\n",
      "Epoch 010: Loss: 1.587\n",
      "Time taken for 1 epoch 756.13 sec\n",
      "\n",
      "Epoch 011: Loss: 1.305\n",
      "Time taken for 1 epoch 752.17 sec\n",
      "\n",
      "Epoch 012: Loss: 1.116\n",
      "Time taken for 1 epoch 750.78 sec\n",
      "\n",
      "Epoch 013: Loss: 0.980\n",
      "Time taken for 1 epoch 748.11 sec\n",
      "\n",
      "Epoch 014: Loss: 0.894\n",
      "Time taken for 1 epoch 749.47 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=15\n",
    "train_loss_results = train_model(encoder_model, decoder_model, num_epochs, dataset_train, dataset_valid, optimizer_obj, loss_obj, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "residential-messaging",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:40.416734Z",
     "iopub.status.busy": "2021-05-19T01:21:40.415956Z",
     "iopub.status.idle": "2021-05-19T01:21:40.606042Z",
     "shell.execute_reply": "2021-05-19T01:21:40.607232Z"
    },
    "id": "residential-messaging",
    "outputId": "86fcde2a-695c-4479-99f7-dabdcd2fc36c",
    "papermill": {
     "duration": 0.278428,
     "end_time": "2021-05-19T01:21:40.607451",
     "exception": false,
     "start_time": "2021-05-19T01:21:40.329023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs), train_loss_results)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-hygiene",
   "metadata": {
    "id": "statutory-hygiene",
    "papermill": {
     "duration": 0.07094,
     "end_time": "2021-05-19T01:21:40.747287",
     "exception": false,
     "start_time": "2021-05-19T01:21:40.676347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-collar",
   "metadata": {
    "id": "phantom-collar",
    "papermill": {
     "duration": 0.06762,
     "end_time": "2021-05-19T01:21:40.881525",
     "exception": false,
     "start_time": "2021-05-19T01:21:40.813905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To test our model, we define a set of sentences in English. To translate the sentences, we first preprocessed and embedded the sentences the same way we did with the training and validation sets. Next, we passed the embedded sentences through the encoder RNN to get the hidden and cell states. Starting with the special `<start>` token, we used this token and the encoder network's final hidden and cell states to get the one-step prediction and updated hidden and cell states from the decoder. Afterward, we created a loop to get the next step prediction and updated hidden and cell states from the decoder, using the most recent hidden and cell states. The loop is terminated when the `<end>` token is emitted or when the sentence has reached a defined maximum length. Finally, we decoded the output token sequence into Portuguese text.\n",
    "\n",
    "The translations that were obtained are reasonably good. An interesting and more complex example was the input ‘are you still at home?’. A question has very clear syntax rules and some of them are language-specific. The returned translation ‘ainda esta em casa?’ showed that the model was able to capture those specificities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "auburn-disco",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:41.026400Z",
     "iopub.status.busy": "2021-05-19T01:21:41.025545Z",
     "iopub.status.idle": "2021-05-19T01:21:41.027951Z",
     "shell.execute_reply": "2021-05-19T01:21:41.027226Z"
    },
    "id": "auburn-disco",
    "papermill": {
     "duration": 0.077601,
     "end_time": "2021-05-19T01:21:41.028110",
     "exception": false,
     "start_time": "2021-05-19T01:21:40.950509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_test = ['that is not safe .',\n",
    "                 'this is my life .',\n",
    "                 'are you still at home ?',\n",
    "                 'it is very cold here .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "painful-relationship",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:41.166055Z",
     "iopub.status.busy": "2021-05-19T01:21:41.165295Z",
     "iopub.status.idle": "2021-05-19T01:21:41.225608Z",
     "shell.execute_reply": "2021-05-19T01:21:41.225152Z"
    },
    "id": "painful-relationship",
    "papermill": {
     "duration": 0.130373,
     "end_time": "2021-05-19T01:21:41.225716",
     "exception": false,
     "start_time": "2021-05-19T01:21:41.095343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_test_emb = embed_english(split_english(tf.data.Dataset.from_tensor_slices((english_test, portuguese_token_train[:len(english_test),:])))).map(map_maxlen).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "further-curtis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:41.312049Z",
     "iopub.status.busy": "2021-05-19T01:21:41.311254Z",
     "iopub.status.idle": "2021-05-19T01:21:41.825346Z",
     "shell.execute_reply": "2021-05-19T01:21:41.826641Z"
    },
    "id": "further-curtis",
    "papermill": {
     "duration": 0.561947,
     "end_time": "2021-05-19T01:21:41.826891",
     "exception": false,
     "start_time": "2021-05-19T01:21:41.264944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_output=[]\n",
    "\n",
    "for x, y in english_test_emb:\n",
    "    hidden_state, cell_state = encoder_model(x)\n",
    "    output_decoder = decoder_model(inputs = np.tile(word_index['<start>'], (1, 1)), hidden_state = hidden_state, cell_state = cell_state)\n",
    "    output=[]\n",
    "    output.append(output_decoder)\n",
    "    for i in range(14):\n",
    "        output_decoder = decoder_model(inputs = tf.math.argmax(output_decoder, axis=2), hidden_state = hidden_state, cell_state = cell_state)\n",
    "        if tf.math.argmax(output_decoder, axis=2).numpy()[0][0] == 2: # <end> character\n",
    "            break\n",
    "        output.append(output_decoder)\n",
    "    total_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "optimum-music",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:41.970198Z",
     "iopub.status.busy": "2021-05-19T01:21:41.969270Z",
     "iopub.status.idle": "2021-05-19T01:21:42.015673Z",
     "shell.execute_reply": "2021-05-19T01:21:42.015260Z"
    },
    "id": "optimum-music",
    "papermill": {
     "duration": 0.118042,
     "end_time": "2021-05-19T01:21:42.015809",
     "exception": false,
     "start_time": "2021-05-19T01:21:41.897767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_output_trans = []\n",
    "for j in range(len(english_test)):\n",
    "    output_trans = []\n",
    "    for i in range(len(total_output[j])):\n",
    "        output_trans.append(tf.math.argmax(total_output[j][i], axis=2).numpy()[0][0])\n",
    "    total_output_trans.append(output_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "administrative-kentucky",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:42.146847Z",
     "iopub.status.busy": "2021-05-19T01:21:42.145604Z",
     "iopub.status.idle": "2021-05-19T01:21:42.147902Z",
     "shell.execute_reply": "2021-05-19T01:21:42.148643Z"
    },
    "id": "administrative-kentucky",
    "papermill": {
     "duration": 0.07098,
     "end_time": "2021-05-19T01:21:42.148846",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.077866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_trans = np.array([np.array(xi) for xi in total_output_trans], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "loved-eclipse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:42.283091Z",
     "iopub.status.busy": "2021-05-19T01:21:42.282255Z",
     "iopub.status.idle": "2021-05-19T01:21:42.285677Z",
     "shell.execute_reply": "2021-05-19T01:21:42.286202Z"
    },
    "id": "loved-eclipse",
    "papermill": {
     "duration": 0.075613,
     "end_time": "2021-05-19T01:21:42.286366",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.210753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "portuguese_trans_batch=[]\n",
    "inv_word_index = {v: k for k, v in word_index.items()}\n",
    "for i in range(output_trans.shape[0]):\n",
    "    portuguese_trans_batch.append(' '.join(list(np.vectorize(inv_word_index.get)(output_trans[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "several-corporation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:42.383451Z",
     "iopub.status.busy": "2021-05-19T01:21:42.382527Z",
     "iopub.status.idle": "2021-05-19T01:21:42.385980Z",
     "shell.execute_reply": "2021-05-19T01:21:42.386378Z"
    },
    "id": "several-corporation",
    "outputId": "5dbb0c26-771e-468b-f95b-67b4b393e4bd",
    "papermill": {
     "duration": 0.049004,
     "end_time": "2021-05-19T01:21:42.386497",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.337493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that is not safe .',\n",
       " 'this is my life .',\n",
       " 'are you still at home ?',\n",
       " 'it is very cold here .']"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(english_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "expected-placement",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-19T01:21:42.470671Z",
     "iopub.status.busy": "2021-05-19T01:21:42.470181Z",
     "iopub.status.idle": "2021-05-19T01:21:42.475514Z",
     "shell.execute_reply": "2021-05-19T01:21:42.475128Z"
    },
    "id": "expected-placement",
    "outputId": "83d81c8a-1e3d-4518-97c6-f4c871c8adf0",
    "papermill": {
     "duration": 0.047984,
     "end_time": "2021-05-19T01:21:42.475627",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.427643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nao e seguro .',\n",
       " 'e a minha vida .',\n",
       " 'ainda esta em casa ?',\n",
       " 'muito frio aqui .']"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_trans_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-journey",
   "metadata": {
    "id": "hungarian-journey",
    "papermill": {
     "duration": 0.040498,
     "end_time": "2021-05-19T01:21:42.555365",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.514867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-vienna",
   "metadata": {
    "id": "innocent-vienna",
    "papermill": {
     "duration": 0.040553,
     "end_time": "2021-05-19T01:21:42.636153",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.595600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The architecture of the NMT model is quite challenging to work with and requires a significant amount of customization, for instance, in its training procedure. We used the principle of Transfer Learning when embedding the English sequences using a pre-trained embedding in a very large corpus. On the other side, we developed our own embedding for the Portuguese language used as input for the decoder network. The encoder and decoder RNNs were kept as simple as possible, as the model is computationally expensive to train.\n",
    "\n",
    "We generated translations from English text to Portuguese without providing anything more than sentence pairs in English and Portuguese to train our model. The model understood affirmation and negation, important syntax distinctions, as in building an interrogative type of clause, and was able to interpret grammatical rules such as subject–auxiliary inversion, often used in English, that do not translate directly to Portuguese.\n",
    "\n",
    "This approach can be extended by increasing the depth of the model with more recurrent layers and the number of units in each layer. Hyperparameters such as the batch size can be tuned to increase accuracy. A wider range of attention strategies can also be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-implementation",
   "metadata": {
    "id": "numerical-implementation",
    "papermill": {
     "duration": 0.040339,
     "end_time": "2021-05-19T01:21:42.716830",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.676491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-springfield",
   "metadata": {
    "id": "solar-springfield",
    "papermill": {
     "duration": 0.040065,
     "end_time": "2021-05-19T01:21:42.797426",
     "exception": false,
     "start_time": "2021-05-19T01:21:42.757361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "[[1]](https://arxiv.org/abs/1609.08144) [Wu et al., 2016] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun,M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., ŁukaszKaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang,W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., andDean, J. (2016). Google’s neural machine translation system: Bridging the gap between humanand machine translation.\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1412.2007) [Jean et al., 2015] Jean, S., Cho, K., Memisevic, R., and Bengio, Y. (2015). On using very largetarget vocabulary for neural machine translation.\n",
    "\n",
    "[[3]](https://towardsdatascience.com/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d) https://towardsdatascience.com/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d\n",
    "\n",
    "[[4]](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128) https://tfhub.dev/google/tf2-preview/nnlm-en-dim128\n",
    "\n",
    "[[5]](https://www.kaggle.com/luisroque/engpor-sentence-pairs) https://www.kaggle.com/luisroque/engpor-sentence-pairs\n",
    "\n",
    "[[6]](https://towardsdatascience.com/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43) https://towardsdatascience.com/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43\n",
    "\n",
    "[[7]](https://arxiv.org/pdf/1409.0473.pdf) [Bahdanau et al., 2016] Bahdanau, D., Cho, K., and Bengio, Y. (2016). Neural machine translationby jointly learning to align and translate.\n",
    "\n",
    "[[8]](https://arxiv.org/pdf/1508.04025v5.pdf) [Luong et al., 2015] Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective approaches to attention-based neural machine translation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook3bd0123b1a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24696.198561,
   "end_time": "2021-05-19T01:21:45.429009",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-18T18:30:09.230448",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
